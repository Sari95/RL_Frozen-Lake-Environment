# -*- coding: utf-8 -*-
"""RL_Frozen-Lake-Environment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OVTDSapWSR1WiW22tZ2uxKX1oMFM41op

# Graded Lab Week 03 – Reinforcement Learning on the Frozen Lake

This Colab notebook is part of the Graded Lab: Value Iteration – Frozen Lake Environment from Week 3 of the Reinforcement Learning course.

The goal of this lab is to implement and compare different reinforcement learning algorithms based on the FrozenLake-v1 environment from OpenAI Gymnasium.

We build on the code provided in previous sessions and progressively extend it. This Document contains the following sessions:

1.   Monte Carlo with a random policy: Estimating state values based on complete episodes under a random behavior.
2.   Incremental Monte Carlo: Updating value estimates incrementally using an ε-greedy exploration strategy.
3. Q-Learning: Applying off-policy temporal-difference learning to compare performance and convergence speed.
4. Conclusion and Final Thoughts

# Basic Environment

Necessary libraries:
- Path and NamedTuple are used for the structured storage of paths and parameters.
- We use tqdm to display progress during the training runs.
- NumPy is required for all numerical calculations.
- We use Matplotlib and Seaborn to graphically display the results and visualise the learned value functions.
- Finally, we import Gymnasium, the central library for Reinforcement Learning (RL) environments. This is where the FrozenLake environment comes from, in which the agent has to move on a grid field from the starting point to the destination without falling into a hole. The states represent the positions on the playing field, and the actions correspond to the possible directions of movement (up, down, left, right).

In the second step, we define the Params class, which contains all the important hyperparameters of the experiment. These parameters control the agent's learning behaviour and determine how strongly future rewards, randomness and learning rate influence behaviour. The parameters learning_rate, gamma and epsilon are particularly relevant for learning behaviour, as they directly control how strongly the agent processes new information, weights future rewards and balances between exploration and exploitation.

In the last step, we build the environment. With the function gym.make(‘FrozenLake-v1’, ...), we create the actual learning environment. The function generate_random_map() creates a random map based on the specified parameter values.

Finally, we output the most important properties of the environment: Map Size specifies the size of the playing field, State Space the number of all possible states (positions) and Action Space the number of possible actions per state.
"""

# Commented out IPython magic to ensure Python compatibility.
# ============================
# General: Basic Environment Setup
# ============================

# Install all required Python packages for the FrozenLake lab setup: pip install numpy matplotlib seaborn tqdm gymnasium

# Imports und Parameter
from pathlib import Path
from typing import NamedTuple
from tqdm import tqdm

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import gymnasium as gym
from gymnasium.envs.toy_text.frozen_lake import generate_random_map

#from misc import *   # (im Original-Notebook vorhanden)

#Setting a uniform plot design
sns.set_theme()
#To display the diagrams directly in the notebook
# %matplotlib inline

# -------------------------------
# Parameter Class as in Lab 02
# -------------------------------

class Params(NamedTuple):
    total_episodes: int    # Total episodes
    learning_rate: float   # Learning rate
    gamma: float           # Discount factor
    epsilon: float         # Exploration probability
    map_size: int          # Size of the grid
    seed: int              # Seed for reproducibility
    is_slippery: bool      # Whether environment is stochastic
    n_runs: int            # Number of runs
    action_size: int       # Number of possible actions
    state_size: int        # Number of possible states
    proba_frozen: float    # Probability that a tile is frozen
    savefig_folder: Path   # Folder for saving plots

# -------------------------------
# Parameter values as in Lab 02
# -------------------------------

params = Params(
    total_episodes = 2000,
    learning_rate  = 0.8,
    gamma          = 0.95,
    epsilon        = 0.1,
    map_size       = 5,
    seed           = 123,
    is_slippery    = False,
    n_runs         = 20,
    action_size    = None,
    state_size     = None,
    proba_frozen   = 0.9,
    savefig_folder = Path("./_static/img"),
)

# Setting seed
rng = np.random.default_rng(params.seed)

# Creating Plot-Folder
params.savefig_folder.mkdir(parents=True, exist_ok=True)

# -------------------------------
# Creating the environment
# -------------------------------

env = gym.make(
    "FrozenLake-v1",
    is_slippery = params.is_slippery,
    render_mode = "rgb_array",
    desc = generate_random_map(
        size = params.map_size,
        p    = params.proba_frozen,
        seed = params.seed
    ),
)

# Defining Action- and State-Space
params = params._replace(action_size = env.action_space.n)
params = params._replace(state_size  = env.observation_space.n)

env.action_space.seed(params.seed)

print("Environment is initialised:")
print(f"- Map size: {params.map_size}x{params.map_size}")
print(f"- State space: {params.state_size}")
print(f"- Action space: {params.action_size}")

"""## 1. Monte Carlo with a random policy - Task 1

In this first part, we implement the Monte Carlo method with a random policy.

Im ersten Schritt definieren wir die Funktion 'monte_carlo_random_policy(). In dieser Funktion geben wir vorher definierte FrozenLake-Umgebung & Parameter mit. Ausserdem setzen wir den Discountfactor gamma auf 0.95 und die Anzahl an Episoden auf 2000. Mit 'first_visit = True' definieren wir, dass nur der erste Besuch pro Episode gezählt wird. Und mit 'track_start_state = True' loggen wir V des Startzustands nach jeder Episode.

Im zweiten Schritt speichern wir in 'v' die aktuelle Schätzung von V(s). Mit 'return_sum' und 'return_count' sammeln wir für jeden Zustand die Summe aller beobachteten Returns. In den Listen protokollieren wir schlussendlich die Episodenlängen, Returns, Erfolge und die Entwicklung von V(s_0).

Mit 'params.seed' setzen wir den Zufalls-Seed genau einmal vor der Episodenschleife. So sind die Ergebnisse reproduzierbar, aber die Folgen einzelner Episoden werden trotzdem zufällig (nicht identisch) sein.

Mit der for-Schleife lassen wir n-Episoden durchlaufen. Wir erzeugen eine komplette Episode von reset() bis 'terminated/truncated'. Pro Zeitschritt speichern wir den Zustand und die Belohnung (für die Rückwärtsberechnung).

Anschliessend berechnen wir den diskontierten Return G rückwärts (ab dem Episodenende). Da wir First-Visit auf True gesetzt haben, wird pro Episode nur der erste Besuch eines Zustands aktualisiert. Wir aktualisieren V(s) als laufenden Durchschnitt der beobachteten Returns. Als nächstes speichern wir V(s_0) nach jeder Episode, so dass wir anschliessend eine Lernkurve ausgeben lassen können.

Am Ende geben wir uns v für die geschätzten Werte V(s) für alle Zustände sowie die Metriken Episodenlänge, Returns, Erfols-Flag und eine Spur von V(s_0) aus.
"""

# ============================
# Task 1: MC Control with random policy
# ============================

import numpy as np
from collections import defaultdict

def monte_carlo_random_policy(
    env,
    params,
    gamma=0.95,
    n_episodes=2000,
    first_visit=True,
    track_start_state=True,
):
    """
    Monte Carlo evaluation under a random policy in FrozenLake.

    Args:
        env: Gymnasium environment (FrozenLake-v1).
        params: Params NamedTuple from the Basic Environment block.
        gamma: Discount factor.
        n_episodes: Number of episodes to sample.
        first_visit: If True, use First-Visit MC; otherwise Every-Visit MC.
        track_start_state: If True, record V(start_state) after each episode.

    Returns:
        V               : np.ndarray of shape [n_states], estimated state values
        epi_lengths     : list[int], length of each episode
        epi_returns     : list[float], undiscounted episode returns
        success_flags   : list[int], 1 if goal reached else 0
        v_start_trace   : list[float], V(start_state) after each episode
    """

    nS = env.observation_space.n

    # Initialize value estimates and accumulators
    V = np.zeros(nS, dtype=float)
    returns_sum = np.zeros(nS, dtype=float)
    returns_count = np.zeros(nS, dtype=float)

    epi_lengths   = []
    epi_returns   = []
    success_flags = []
    v_start_trace = []

    # Important for reproducibility: setting the seed ONCE before the loop
    state, _ = env.reset(seed=params.seed)
    start_state = state

    for ep in tqdm(range(n_episodes), desc="MC (random policy)"):
        # --- Generating one episode under a random policy ---
        if ep > 0:
            # Do NOT pass the seed again, otherwise episodes repeat deterministically
            state, _ = env.reset()

        states = []
        rewards = []
        done = False
        steps = 0

        while not done:
            action = env.action_space.sample()  # random action (pure exploration)
            next_state, reward, terminated, truncated, info = env.step(action)
            states.append(state)
            rewards.append(reward)
            state = next_state
            steps += 1
            done = terminated or truncated

        # Episode-level stats (FrozenLake rewards 1 only when reaching the goal)
        G_undiscounted = float(sum(rewards))
        epi_lengths.append(steps)
        epi_returns.append(G_undiscounted)
        success_flags.append(1 if G_undiscounted > 0.0 else 0)

        # --- Monte Carlo updates (backward return computation) ---
        visited = set()
        G = 0.0
        for t in reversed(range(len(states))):
            G = gamma * G + rewards[t]
            s_t = states[t]

            if first_visit:
                # First-Visit MC: update only at the first occurrence in this episode
                if s_t in visited:
                    continue
                visited.add(s_t)

            returns_sum[s_t]  += G
            returns_count[s_t] += 1.0
            V[s_t] = returns_sum[s_t] / max(1.0, returns_count[s_t])

        if track_start_state:
            v_start_trace.append(V[start_state])

    return V, epi_lengths, epi_returns, success_flags, v_start_trace

"""We use this small help function to smooth noisy curves. The function calculates moving averages over a window of length K. We do this before plotting, as Monte Carlo estimates have high variance under random policy."""

# Smoothing for noisy learning curves
def moving_average(x, k=50):
    """Simple moving average for smoother learning curves."""
    x = np.asarray(x, dtype=float)
    if len(x) < k:
        return x
    cumsum = np.cumsum(np.insert(x, 0, 0.0))
    return (cumsum[k:] - cumsum[:-k]) / k

"""We start the training with the following code: The function executes all episodes in FrozenLake under a random policy and estimates the state values V(s) using Monte Carlo (first visit) with the discount factor Gamma.

In the output, we see the progress bar until all 2000 episodes have been run through.
"""

#Training
V_random, epi_len, epi_ret, success, v0_trace = monte_carlo_random_policy(
    env=env,
    params=params,
    gamma=params.gamma,
    n_episodes=params.total_episodes,
    first_visit=True  # toggle to False to compare Every-Visit MC
)

"""Next, we want to visualise the results: The learning curves, the trajectory lenghts and the estimated V(s)."""

#Control
print("Mean return:", np.mean(epi_ret))
print("Mean success:", np.mean(success))

# Learning curves: average return and success rate (smoothed)
plt.figure()
plt.plot(moving_average(success, k=50), color="#3EB489", label="Success rate (SMA 50)")
plt.xlabel("Episode")
plt.ylabel("Value")
plt.title("Learning Curves for Monte Carlo Evaluation (Random Policy)")
plt.legend()
plt.show()

# Trajectory length over episodes (smoothed)
vals = moving_average(epi_len, k=50)
upper = max(vals) * 1.1  # kclaudleiner Headroom
plt.figure()
plt.plot(vals, color="#3EB489")
plt.xlabel("Episode"); plt.ylabel("Trajectory length")
plt.title("Average Trajectory Length over Episodes (SMA 50)")
plt.ylim(0, upper)
plt.grid(alpha=0.2)
plt.tight_layout(); plt.show()

"""Value function learned by each algorithm as heatmap:"""

V_grid = V_random.reshape(params.map_size, params.map_size)
sns.heatmap(V_grid, annot=True, cmap="YlGnBu")
plt.title("Estimated State-Value Function V(s) – Monte Carlo (Random Policy)")
plt.show()

"""After printing out the results as plots, we do the same procedure for an 11x11-Grid to compare it with the 5x5-Grid."""

# =========================
# The same for 11×11 Environment & Train
# =========================
params_11 = params._replace(map_size=11)

env_11 = gym.make(
    "FrozenLake-v1",
    is_slippery=params_11.is_slippery,
    render_mode="rgb_array",
    desc=generate_random_map(
        size=params_11.map_size,
        p=params_11.proba_frozen,
        seed=params_11.seed
    ),
)
params_11 = params_11._replace(action_size=env_11.action_space.n,
                               state_size =env_11.observation_space.n)
env_11.action_space.seed(params_11.seed)

V_random_11, epi_len_11, epi_ret_11, success_11, v0_trace_11 = monte_carlo_random_policy(
    env=env_11,
    params=params_11,
    gamma=params_11.gamma,
    n_episodes=params_11.total_episodes,
    first_visit=True
)

# Colours & Smoothing
COL_5  = "#3EB489"  # Mintgrün (5×5)
COL_11 = "#0000ff"  # Blau (11×11)
K = 50

# =========================
# 1) Learning Curve: Shows only the Success Rate
#    -> Y-axis identical to that in the 5×5 plot (0 ... 0.12)
# =========================
plt.figure(figsize=(8,5))
plt.plot(moving_average(success,    K),  label="Success Rate (5×5)",  color=COL_5)
plt.plot(moving_average(success_11, K),  label="Success Rate (11×11)", color=COL_11)
plt.xlabel("Episode")
plt.ylabel("Success Rate")
plt.title("Success Rate over Episodes — Monte Carlo (Random Policy)")
plt.ylim(0, 0.12)   # Skala gleich wie im 5x5-Plot
plt.legend()
plt.grid(alpha=0.2)
plt.tight_layout()
plt.show()

# =========================
# 2) Trajectory Length:
# =========================
vals_5  = moving_average(epi_len,    K)
vals_11 = moving_average(epi_len_11, K)
ymax = max(np.max(vals_5), np.max(vals_11)) * 1.1

plt.figure(figsize=(8,5))
plt.plot(vals_5,  label="Trajectory Length (5×5)",  color=COL_5)
plt.plot(vals_11, label="Trajectory Length (11×11)", color=COL_11)
plt.xlabel("Episode")
plt.ylabel("Trajectory Length")
plt.title("Average Trajectory Length over Episodes (SMA 50)")
plt.ylim(0, ymax)
plt.legend()
plt.grid(alpha=0.2)
plt.tight_layout()
plt.show()

# =========================
# 3) Heatmaps: V(s) for 5×5 & 11×11
# =========================
V_grid_5  = V_random.reshape(5, 5)
V_grid_11 = V_random_11.reshape(11, 11)

fig, axs = plt.subplots(1, 2, figsize=(12, 5))  # 1 Reihe, 2 Spalten

# --- 5×5 ---
sns.heatmap(V_grid_5, annot=True, fmt=".2f", cmap="YlGnBu",
            cbar_kws={"label": "V(s)"}, ax=axs[0])
axs[0].set_title("Estimated State-Value Function V(s)\nMonte Carlo (Random Policy), 5×5")

# --- 11×11 ---
#For better visibility, “annot=False” can be set if required. Then the numbers will not be displayed, only the colours.
sns.heatmap(V_grid_11, annot=True, fmt=".2f", cmap="YlGnBu",
            cbar_kws={"label": "V(s)"}, ax=axs[1])
axs[1].set_title("Estimated State-Value Function V(s)\nMonte Carlo (Random Policy), 11×11")

plt.tight_layout()
plt.show()

"""## 2. Incremental Monte Carlo - Task 2

In the second part, we extent the Monte Carlo algorithm above: Incremental Monte Carlo updates the value estimates incrementally, reducing the need to wait until the end of all episodes to update values. This means that instead of randomly sampling from all possible actions, we sample according to the estimated value function. This should lead to more efficient learning.
"""

# ============================
# Task 2: Incremental MC Control (ε-greedy)
# ============================

import numpy as np
from tqdm import tqdm

# --- ε-greedy Action selection (with random tie-break) ---
def epsilon_greedy_action(Q_row, epsilon, rng, nA):
    if rng.random() < epsilon:
        return int(rng.integers(nA))
    max_q = np.max(Q_row)
    best = np.flatnonzero(Q_row == max_q)
    return int(rng.choice(best))

# --- Incremental Monte Carlo Control (on-policy, First-Visit) ---
def monte_carlo_incremental_control(
    env,
    params,
    gamma=0.95,
    n_episodes=2000,
    alpha=0.1,          # feste Schrittweite; None => 1/N(s,a)
    epsilon=0.1,        # ε-greedy Exploration
    epsilon_decay=None, # z.B. 0.999 oder None
    epsilon_min=0.01,
    first_visit=True,
    track_start_state=True,
):
    rng = np.random.default_rng(params.seed)
    nS, nA = env.observation_space.n, env.action_space.n

    Q = np.zeros((nS, nA), dtype=float)
    N = np.zeros((nS, nA), dtype=int)  # für alpha=None

    epi_lengths, epi_returns, success_flags, v_start_trace = [], [], [], []

    state, _ = env.reset(seed=params.seed)
    start_state = state
    eps = epsilon

    for ep in tqdm(range(n_episodes), desc="Incremental MC Control (ε-greedy)"):
        if ep > 0:
            state, _ = env.reset()

        states, actions, rewards = [], [], []
        done, steps = False, 0

        # --- Generating episodes: ε-greedy Policy w.r.t. Q ---
        while not done:
            a = epsilon_greedy_action(Q[state], eps, rng, nA)
            ns, r, terminated, truncated, _ = env.step(a)

            states.append(state)
            actions.append(a)
            rewards.append(r)

            state = ns
            steps += 1
            done = terminated or truncated

        # Logging per episode
        G_undisc = float(sum(rewards))
        epi_lengths.append(steps)
        epi_returns.append(G_undisc)
        success_flags.append(1 if G_undisc > 0.0 else 0)

        # --- MC-Update backwards ---
        visited_sa = set()
        G = 0.0
        for t in reversed(range(len(states))):
            G = gamma * G + rewards[t]
            s_t, a_t = states[t], actions[t]

            if first_visit:
                if (s_t, a_t) in visited_sa:
                    continue
                visited_sa.add((s_t, a_t))

            if alpha is None:
                N[s_t, a_t] += 1
                step = 1.0 / N[s_t, a_t]
            else:
                step = alpha

            Q[s_t, a_t] += step * (G - Q[s_t, a_t])

        if track_start_state:
            v_start_trace.append(np.max(Q[start_state]))

        if epsilon_decay is not None:
            eps = max(epsilon_min, eps * epsilon_decay)

    return Q, epi_lengths, epi_returns, success_flags, v_start_trace

# ============================
# Training 5x5
# ============================
Q_inc, epi_len_inc, epi_ret_inc, success_inc, v0_trace_inc = monte_carlo_incremental_control(
    env=env,
    params=params,
    gamma=params.gamma,
    n_episodes=params.total_episodes,
    alpha=0.1,              # oder None für 1/N(s,a)
    epsilon=params.epsilon,
    epsilon_decay=0.999,    # optionales Decay
    epsilon_min=0.01,
    first_visit=True
)

# Derived quantities for plots/answers
V_inc  = Q_inc.max(axis=1)               # V(s) = max_a Q(s,a)
pi_inc = Q_inc.argmax(axis=1)            # greedy Policy (ohne ε)

# ========= Plots Task 2 for the 5×5-grid =========
K = 50
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Success Rate (only 1 curve as Return≈Success in FrozenLake)
plt.figure(figsize=(8,5))
plt.plot(moving_average(success_inc, K), label="Success rate (SMA 50)", color="#3EB489")
plt.xlabel("Episode"); plt.ylabel("Success Rate")
plt.title("Success Rate over Episodes — Incremental Monte Carlo (ε-greedy)")
plt.ylim(0, 1.0)
plt.legend(); plt.grid(alpha=0.2); plt.tight_layout(); plt.show()

# 2) Trajectory Length
vals_len = moving_average(epi_len_inc, K)
plt.figure(figsize=(8,5))
plt.plot(vals_len, color="#3EB489")
plt.xlabel("Episode"); plt.ylabel("Trajectory Length")
plt.title("Average Trajectory Length over Episodes (SMA 50) — Incremental MC")
plt.ylim(0, max(vals_len)*1.1)
plt.grid(alpha=0.2); plt.tight_layout(); plt.show()

# 3) Value-Heatmap V(s) = max_a Q(s,a)
V_grid_inc = V_inc.reshape(params.map_size, params.map_size)
plt.figure(figsize=(5.6,4.8))
sns.heatmap(V_grid_inc, annot=True, fmt=".2f", cmap="YlGnBu",
            cbar_kws={"label":"V(s)"})
plt.title("Estimated State-Value Function V(s) — Incremental MC (ε-greedy)")
plt.tight_layout(); plt.show()

# ========= 11×11 Comparison =========
params_11 = params._replace(map_size=11)
env_11 = gym.make(
    "FrozenLake-v1",
    is_slippery=params_11.is_slippery,
    render_mode="rgb_array",
    desc=generate_random_map(size=params_11.map_size, p=params_11.proba_frozen, seed=params_11.seed),
)
params_11 = params_11._replace(action_size=env_11.action_space.n, state_size=env_11.observation_space.n)
env_11.action_space.seed(params_11.seed)

Q_inc_11, epi_len_inc_11, epi_ret_inc_11, success_inc_11, _ = monte_carlo_incremental_control(
    env=env_11,
    params=params_11,
    gamma=params_11.gamma,
    n_episodes=params_11.total_episodes,
    alpha=0.1,
    epsilon=params_11.epsilon,
    epsilon_decay=0.999,
    epsilon_min=0.01,
    first_visit=True
)
V_inc_11 = Q_inc_11.max(axis=1)

# Plots to compare
COL_5, COL_11 = "#3EB489", "#0000ff"

plt.figure(figsize=(8,5))
plt.plot(moving_average(success_inc,    K), label="Success Rate (5×5)",  color=COL_5)
plt.plot(moving_average(success_inc_11, K), label="Success Rate (11×11)", color=COL_11)
plt.xlabel("Episode"); plt.ylabel("Success Rate")
plt.title("Success Rate over Episodes — Incremental MC (ε-greedy): 5×5 vs 11×11")
# Set the same upper limit as for the 5×5 plot (optional):
# plt.ylim(0, max(moving_average(success_inc, K))*1.0)
plt.ylim(0, 1.0)
plt.legend(); plt.grid(alpha=0.2); plt.tight_layout(); plt.show()

vals_5  = moving_average(epi_len_inc,    K)
vals_11 = moving_average(epi_len_inc_11, K)
ymax = max(vals_5.max(), vals_11.max()) * 1.1
plt.figure(figsize=(8,5))
plt.plot(vals_5,  label="Trajectory Length (5×5)",  color=COL_5)
plt.plot(vals_11, label="Trajectory Length (11×11)", color=COL_11)
plt.xlabel("Episode"); plt.ylabel("Trajectory Length")
plt.title("Average Trajectory Length (SMA 50) — Incremental MC: 5×5 vs 11×11")
plt.ylim(0, ymax)
plt.legend(); plt.grid(alpha=0.2); plt.tight_layout(); plt.show()

# =========================
# Heatmaps: Incremental MC (5×5 & 11×11 side by side)
# =========================
V_grid_5  = V_inc.reshape(params.map_size, params.map_size)
V_grid_11 = V_inc_11.reshape(params_11.map_size, params_11.map_size)

# Colour Scala
vmin = min(V_inc.min(), V_inc_11.min())
vmax = max(V_inc.max(), V_inc_11.max())

fig, axs = plt.subplots(1, 2, figsize=(12, 5))  # 1 Reihe, 2 Spalten

# --- 5×5 ---
sns.heatmap(
    V_grid_5, annot=True, fmt=".2f", cmap="YlGnBu",
    cbar_kws={"label": "V(s)"}, vmin=vmin, vmax=vmax, ax=axs[0]
)
axs[0].set_title("Estimated State-Value Function V(s)\nIncremental MC (ε-greedy), 5×5")

# --- 11×11 ---
sns.heatmap(
    V_grid_11, annot=True, fmt=".2f", cmap="YlGnBu",
    cbar_kws={"label": "V(s)"}, vmin=vmin, vmax=vmax, ax=axs[1]
)
axs[1].set_title("Estimated State-Value Function V(s)\nIncremental MC (ε-greedy), 11×11")

plt.tight_layout()
plt.show()

# ========= KPIs for the report =========
print("Success rate (mean over episodes): 5×5 =", np.mean(success_inc),
      "| 11×11 =", np.mean(success_inc_11))
print("Avg trajectory length (mean):      5×5 =", np.mean(epi_len_inc),
      "| 11×11 =", np.mean(epi_len_inc_11))

"""## 3. Q-Learning - Task 3"""

class Qlearning:
    def __init__(self, learning_rate, gamma, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.reset_qtable()
        self.qtable = np.zeros((self.state_size, self.action_size)) #C 25X4 table of the 25 states and 4 posible actions


    def update(self, state, action, reward, new_state):
        TD_target = reward + self.gamma * np.max(self.qtable[new_state, :])
        TD_error = TD_target - self.qtable[state, action]
        return self.qtable[state, action] + self.learning_rate * TD_error

    def reset_qtable(self):
        """Reset the Q-table."""
        self.qtable = np.zeros((self.state_size, self.action_size))



class EpsilonGreedy:
    def __init__(self, epsilon, epsilon_min, decay_rate):
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.decay_rate = decay_rate

    def choose_action(self, action_space, state, qtable):
        if np.random.rand() < self.epsilon:
            action = action_space.sample()
        else:
            # Get all actions that have the maximum Q-value
            max_q = np.max(qtable[state, :])
            best_actions = np.where(qtable[state, :] == max_q)[0]
            # Randomly choose among the best actions
            action = np.random.choice(best_actions)
        return action

    def decay_epsilon(self):
      self.epsilon = max(self.epsilon_min,  self.epsilon * self.decay_rate)

def run_env(env, params):
    rewards = np.zeros((params.total_episodes, params.n_runs))
    steps = np.zeros((params.total_episodes, params.n_runs))
    episodes = np.arange(params.total_episodes)
    qtables = np.zeros((params.n_runs, params.state_size, params.action_size))
    all_states = []
    all_actions = []

    for run in range(params.n_runs):  # Run several times to account for stochasticity
        learner.reset_qtable()  # Reset the Q-table between runs

        for episode in tqdm(
            episodes, desc=f"Run {run}/{params.n_runs} - Episodes", leave=False
        ):
            state = env.reset(seed=params.seed)[0]  # Reset the environment
            step = 0
            done = False
            total_rewards = 0

            while not done:
                action = explorer.choose_action(
                    action_space=env.action_space, state=state, qtable=learner.qtable
                )

                # Log all states and actions
                all_states.append(state)
                all_actions.append(action)

                # Take the action (a) and observe the outcome state(s') and reward (r)
                new_state, reward, terminated, truncated, info = env.step(action)

                done = terminated or truncated

                learner.qtable[state, action] = learner.update(
                    state, action, reward, new_state
                )

                total_rewards += reward
                step += 1

                # Our new state is state
                state = new_state

            # Log all rewards and steps
            rewards[episode, run] = total_rewards
            steps[episode, run] = step
            explorer.decay_epsilon()
        qtables[run, :, :] = learner.qtable

    return rewards, steps, episodes, qtables, all_states, all_actions


def run_env_11(env, params):
    rewards = np.zeros((params.total_episodes, params.n_runs))
    steps = np.zeros((params.total_episodes, params.n_runs))
    episodes = np.arange(params.total_episodes)
    qtables = np.zeros((params.n_runs, params.state_size, params.action_size))
    all_states = []
    all_actions = []

    for run in range(params.n_runs):  # Run several times to account for stochasticity
        learner_11.reset_qtable()  # Reset the Q-table between runs

        for episode in tqdm(
            episodes, desc=f"Run {run}/{params.n_runs} - Episodes", leave=False
        ):
            state = env.reset(seed=params.seed)[0]  # Reset the environment
            step = 0
            done = False
            total_rewards = 0

            while not done:
                action = explorer.choose_action(
                    action_space=env.action_space, state=state, qtable=learner_11.qtable
                )

                # Log all states and actions
                all_states.append(state)
                all_actions.append(action)

                # Take the action (a) and observe the outcome state(s') and reward (r)
                new_state, reward, terminated, truncated, info = env.step(action)

                done = terminated or truncated

                learner_11.qtable[state, action] = learner_11.update(
                    state, action, reward, new_state
                )

                total_rewards += reward
                step += 1

                # Our new state is state
                state = new_state

            # Log all rewards and steps
            rewards[episode, run] = total_rewards
            steps[episode, run] = step
            explorer.decay_epsilon()
        qtables[run, :, :] = learner_11.qtable

    return rewards, steps, episodes, qtables, all_states, all_actions

    rewards, steps, episodes, qtables, all_states, all_actions = run_env(
    env = env,
    params = params
)

rewards_11, steps_11, episodes_11, qtables_11, all_states_11, all_actions_11 = run_env_11(
    env = env_11,
    params = params_11
)

plt.plot(np.mean(rewards, axis=1))
plt.xlabel("Episode")
plt.ylabel("Average Reward")
plt.title("Learning Progress (Q-Learning)")
plt.show()


# ========================
# Combined Plots: 5x5 vs 11x11
# ========================
K = 50

fig, axes = plt.subplots(3, 1, figsize=(8, 14))
plt.subplots_adjust(hspace=0.4)

# ========================
# Plot 1: Success Rate
# ========================
axes[0].plot(moving_average(rewards.mean(axis=1), K), label="5x5 (SMA 50)", color="#ff7f0e")
axes[0].plot(moving_average(rewards_11.mean(axis=1), K), label="11x11 (SMA 50)", color="#1f77b4")
axes[0].set_xlabel("Episode")
axes[0].set_ylabel("Success Rate")
axes[0].set_title("Learning Progress — Q-Learning (ε-greedy)")
axes[0].set_ylim(0, 1.0)
axes[0].legend()
axes[0].grid(alpha=0.3)

# ========================
# Plot 2: Average Trajectory Length
# ========================
axes[1].plot(moving_average(steps.mean(axis=1), K), label="5x5 (SMA 50)", color="#ff7f0e")
axes[1].plot(moving_average(steps_11.mean(axis=1), K), label="11x11 (SMA 50)", color="#1f77b4")

axes[1].axhline(y=8, color="gray", linestyle="--", linewidth=1, alpha=0.7, label="Target: 8 steps")
axes[1].axhline(y=20, color="gray", linestyle=":", linewidth=1, alpha=0.7, label="Target: 20 steps")

axes[1].set_xlabel("Episode")
axes[1].set_ylabel("Average Steps per Episode")
axes[1].set_title("Average Trajectory Length — Q-Learning")
axes[1].legend()
axes[1].grid(alpha=0.3)

# ========================
# Plot 3: State-Value Heatmaps
# ========================
Q_avg_5 = qtables.mean(axis=0)
V_5 = Q_avg_5.max(axis=1)
V_grid_5 = V_5.reshape(params.map_size, params.map_size)

Q_avg_11 = qtables_11.mean(axis=0)
V_11 = Q_avg_11.max(axis=1)
V_grid_11 = V_11.reshape(params_11.map_size, params_11.map_size)

# Plot heatmaps side by side within one row
heatmap_fig, heatmap_axes = plt.subplots(1, 2, figsize=(13, 6))
sns.heatmap(V_grid_5, annot=True, fmt=".2f", cmap="YlGnBu",
            cbar_kws={"label": "V(s)"}, ax=heatmap_axes[0])
heatmap_axes[0].set_title("State-Value (5x5)")

sns.heatmap(V_grid_11, annot=True, fmt=".2f", cmap="YlGnBu",
            cbar_kws={"label": "V(s)"}, ax=heatmap_axes[1])
heatmap_axes[1].set_title("State-Value (11x11)")

heatmap_fig.suptitle("Estimated State-Value Function — Q-Learning (ε-greedy)", fontsize=14)
plt.tight_layout()
plt.show()

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))

# Plot 5x5 environment
ax1.imshow(env.render())
ax1.set_title("FrozenLake 5x5 Environment", fontsize=16, fontweight='bold')
ax1.axis('off')

# Plot 11x11 environment
ax2.imshow(env_11.render())
ax2.set_title("FrozenLake 11x11 Environment", fontsize=16, fontweight='bold')
ax2.axis('off')

# Add overall title and legend
plt.suptitle("Environment", fontsize=18, fontweight='bold', y=0.95)
plt.tight_layout()


#Code
